{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cj8bcxFX85UO"
      },
      "source": [
        "# Práctica Final: detección de mensajes troll\n",
        "\n",
        "\n",
        "\n",
        "En los últimos años Twitch se ha consolidad como uno de los principales medios de comunicación especialmente para las generaciones más jóvenes.\n",
        "\n",
        "Al tratarse de una plataforma participativa en la que los usuarios pueden poner comentarios durante y posteriormente a las emisiones. Entre estos comentarios han aparecido como siempre comentarios ofensisvos.\n",
        "\n",
        "En esta práctica construiremos una Inteligencia Artificial capaz de clasificar esos mensajes troll.\n",
        "\n",
        "Durante la práctica entrenaremos el modelo de Deep Learning y lo desplegaremos para inferencia en batch, la más habitual actualmente dentro de la industria:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AI5vlvwU9FAv"
      },
      "source": [
        "## Teoría\n",
        "\n",
        "**1. ¿Qué es Apache Beam?**\n",
        "\n",
        "Apache Beam es un framework de trabajo que permite definir pipelines de procesado de datos enfocadas a la paralelización de procesos.\n",
        "\n",
        "Beam es una abstracción por lo que los detalles de implementación son gestionados por los motores de ejecución (Flink, DataFlow...)\n",
        "\n",
        "Es un framework compatible con distintos lenguajes de programación como Java, Python o Go.\n",
        "\n",
        "**2. ¿Cuáles son las diferentes formas de desplegar un modelo?**\n",
        "\n",
        "Distinguimos cuatro opciones:\n",
        "\n",
        "* Despligue en Batch. Desplegamos el modelo para hacer predicciones de grandes cantidades de datos en un momento puntual.\n",
        "\n",
        "* Despligue online. Permite la disponibilización del modelo a demanda por el ususario de manera que se pueden realizar predicciones mediante llamdas a APIs.\n",
        "\n",
        "* Despliegue en streaming Despligue pensando en la recepción continua de datos de manera que nuestro servidor es capaz de procesar un flujo continuo e ininterrumpido de datos en tiempo real.\n",
        "\n",
        "* ML Automatizado. Es el estado más avanzado en el desarrollo de MLOPS en el que el modelo es monitoeado y reentrenado de manera automática.\n",
        "\n",
        "**3. ¿Cuál es la principal diferencia entre la inferencia en batch y la inferencia en streaming?**\n",
        "\n",
        "La inferencia en batch procesa grandes cantidades de datos en momentos puntuales y normalmente programados de antemano frente al procesamiento en streming cuya infrastructura está diseñada para estar constantemente en funcionamiento emitiendo predicciones de manera continua y en tiempo real (o casi).\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hgE9XB8ACVmQ"
      },
      "source": [
        "# Configuración de nuestro proyecto en GCP\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "id": "zAorVw7RCT9C",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a1b8e50f-fdd7-43cd-f7eb-1896ccc2a3ee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Updated property [core/project].\n"
          ]
        }
      ],
      "source": [
        "PROJECT_ID = \"trolls-419509\" #@param {type:\"string\"}\n",
        "! gcloud config set project $PROJECT_ID"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "id": "hbAl8pSkCXCm"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "\n",
        "# If you are running this notebook in Colab, run this cell and follow the\n",
        "# instructions to authenticate your GCP account. This provides access to your\n",
        "# Cloud Storage bucket and lets you submit training jobs and prediction\n",
        "# requests.\n",
        "\n",
        "if 'google.colab' in sys.modules:\n",
        "  from google.colab import auth as google_auth\n",
        "  google_auth.authenticate_user()\n",
        "\n",
        "# If you are running this notebook locally, replace the string below with the\n",
        "# path to your service account key and run this cell to authenticate your GCP\n",
        "# account.\n",
        "else:\n",
        "  %env GOOGLE_APPLICATION_CREDENTIALS ''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p5Xoej7vH0X-"
      },
      "source": [
        "Creamos el bucket mediante la consola y una vez creado lo indicamos en la variable:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "id": "n6Zpx14FCfXy"
      },
      "outputs": [],
      "source": [
        "BUCKET_NAME = \"trolls\" #@param {type:\"string\"}\n",
        "REGION = \"europe-west1\" #@param {type:\"string\"}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "id": "lE4V-Xt0ChSb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6bc2f1ca-b062-4af2-d1cf-79e396296e29"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating gs://trolls/...\n",
            "ServiceException: 409 A Cloud Storage bucket named 'trolls' already exists. Try another name. Bucket names must be globally unique across all Google Cloud projects, including those outside of your organization.\n"
          ]
        }
      ],
      "source": [
        "! gsutil mb -l $REGION gs://$BUCKET_NAME"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! gsutil ls -al gs://$BUCKET_NAME"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QA9lyY7toyLE",
        "outputId": "53dcebbd-c47e-43e8-856a-48136d34bde4"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "         1  2023-06-27T06:23:31Z  gs://trolls/1#1687847011495817  metageneration=1\n",
            "         0  2024-04-06T09:44:33Z  gs://trolls/data.json#1712396673065087  metageneration=1\n",
            "TOTAL: 2 objects, 1 bytes (1 B)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3D8Qw14y_nRa"
      },
      "source": [
        "# Entrenamiento e inferencia en Batch\n",
        "\n",
        "## Preparación\n",
        "\n",
        "Para esta primera parte se va a utilizar [Tweets Dataset for Detection of Cyber-Trolls](https://www.kaggle.com/dataturks/dataset-for-detection-of-cybertrolls). El objetivo es desarrollar un clasificador binario para detectar si el mensaje recibido es troll (1) o no (0). **Las métricas obtenidas del entrenamiento y la inferencia no se tendrán en cuenta para la evaluación de la práctica, la importancia está en la arquitectura de la solución**, es decir, lo importante no es que nuestro modelo detecte correctamente los tweets de trolls si no que funcione y sea capaz de hacer inferencias.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qM_dh-471gIq"
      },
      "source": [
        "A continuación, se van a subir los datos de entrenamiento al bucket del proyecto que se haya creado. **Importante:** crea el bucket en una única región. Os dejo disponibilizado el dataset en un bucket de acceso público:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sqp4L_nmUjAc",
        "outputId": "887c32c3-1441-4676-86b1-56ec18f452ea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gdown in /usr/local/lib/python3.10/dist-packages (4.7.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from gdown) (3.13.3)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.10/dist-packages (from gdown) (2.31.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from gdown) (1.16.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from gdown) (4.66.2)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from gdown) (4.12.3)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->gdown) (2.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (2024.2.2)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (1.7.1)\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1dTaKofC9ZcMWa5cVtGLDFkEnbc4hiPJr\n",
            "To: /content/batch/twitter-sentiment-batch/dataset-cybertrolls.json\n",
            "100% 2.76M/2.76M [00:00<00:00, 148MB/s]\n"
          ]
        }
      ],
      "source": [
        "%pip install gdown\n",
        "! gdown \"1dTaKofC9ZcMWa5cVtGLDFkEnbc4hiPJr\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vCQQ9j2I11tg"
      },
      "source": [
        "Ahora se crea el directorio dónde vas a desarrollar esta primera parte de la práctica."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "id": "PsblBlJ6RrGm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1b621ad3-ef9a-414b-b04f-3f8eee87921c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mkdir: cannot create directory ‘/content/batch’: File exists\n"
          ]
        }
      ],
      "source": [
        "%mkdir /content/batch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nyK51quU1_oi"
      },
      "source": [
        "Se establece el directorio de trabajo que hemos creado."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "id": "s7ybSaotRwkP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9ab82026-3cf1-4747-d981-2d6ec44ac557"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/batch\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "# Set the working directory to the sample code directory\n",
        "%cd /content/batch\n",
        "\n",
        "WORK_DIR = os.getcwd()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd twitter-sentiment-batch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YslMM1RS0BiL",
        "outputId": "3856f718-7db8-46c8-e122-60d260a07453"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/batch/twitter-sentiment-batch\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kvi7511iz7BH",
        "outputId": "17bbc5ef-fb3b-468f-a8c2-bdf2a02e3897"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "data.json\t\t  predict-work.py     requirements.txt\ttrainer\n",
            "dataset-cybertrolls.json  preprocess.py       setup.py\t\ttwitter-sentiment-batch\n",
            "predict.py\t\t  preprocess-work.py  tokenizer.pkl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4NSd4bAo2aj_"
      },
      "source": [
        "Ahora se descargarán los datos en el workspace de Colab para trabajar en local."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "id": "qOmUJSg1JCgg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "422e6338-af22-458b-b29b-4e1da7822ebf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eRoW6zyg2kEz"
      },
      "source": [
        "Se establecen las dependencias que se usarán en la práctica. Se pueden añadir y quitar las dependencias que no se usen o viceversa."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile requirements.txt\n",
        "\n",
        "apache-beam[gcp]\n",
        "tensorflow\n",
        "gensim==3.6.0\n",
        "fsspec==0.8.4\n",
        "gcsfs==0.7.1\n",
        "numpy==1.20.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SP7-L1hzKB8c",
        "outputId": "3f19f698-5535-4fcd-bfef-ce8dcad24cf3"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting requirements.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yIl8lrPp2x2j"
      },
      "source": [
        "Instalamos las dependencias. **No olvides reiniciar el entorno al instalar y establecer las variables y credenciales de GCP al arrancar.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "id": "74dHb1iQ0UNN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "376b060b-4401-42e2-8689-adf32b7fb25b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: apache-beam[gcp] in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 2)) (2.55.0)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 3)) (2.15.0)\n",
            "Collecting gensim==3.6.0 (from -r requirements.txt (line 4))\n",
            "  Using cached gensim-3.6.0-cp310-cp310-linux_x86_64.whl\n",
            "Collecting fsspec==0.8.4 (from -r requirements.txt (line 5))\n",
            "  Using cached fsspec-0.8.4-py3-none-any.whl (91 kB)\n",
            "Collecting gcsfs==0.7.1 (from -r requirements.txt (line 6))\n",
            "  Using cached gcsfs-0.7.1-py2.py3-none-any.whl (20 kB)\n",
            "Collecting numpy==1.20.0 (from -r requirements.txt (line 7))\n",
            "  Using cached numpy-1.20.0.zip (8.0 MB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.10/dist-packages (from gensim==3.6.0->-r requirements.txt (line 4)) (1.11.4)\n",
            "Requirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.10/dist-packages (from gensim==3.6.0->-r requirements.txt (line 4)) (1.16.0)\n",
            "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.10/dist-packages (from gensim==3.6.0->-r requirements.txt (line 4)) (6.4.0)\n",
            "Requirement already satisfied: google-auth>=1.2 in /usr/local/lib/python3.10/dist-packages (from gcsfs==0.7.1->-r requirements.txt (line 6)) (2.27.0)\n",
            "Requirement already satisfied: google-auth-oauthlib in /usr/local/lib/python3.10/dist-packages (from gcsfs==0.7.1->-r requirements.txt (line 6)) (1.2.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from gcsfs==0.7.1->-r requirements.txt (line 6)) (2.31.0)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from gcsfs==0.7.1->-r requirements.txt (line 6)) (4.4.2)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from gcsfs==0.7.1->-r requirements.txt (line 6)) (3.9.3)\n",
            "Requirement already satisfied: crcmod<2.0,>=1.7 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]->-r requirements.txt (line 2)) (1.7)\n",
            "Requirement already satisfied: orjson<4,>=3.9.7 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]->-r requirements.txt (line 2)) (3.10.0)\n",
            "Requirement already satisfied: dill<0.3.2,>=0.3.1.1 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]->-r requirements.txt (line 2)) (0.3.1.1)\n",
            "Requirement already satisfied: cloudpickle~=2.2.1 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]->-r requirements.txt (line 2)) (2.2.1)\n",
            "Requirement already satisfied: fastavro<2,>=0.23.6 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]->-r requirements.txt (line 2)) (1.9.4)\n",
            "Requirement already satisfied: fasteners<1.0,>=0.3 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]->-r requirements.txt (line 2)) (0.19)\n",
            "Requirement already satisfied: grpcio!=1.48.0,<2,>=1.33.1 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]->-r requirements.txt (line 2)) (1.62.1)\n",
            "Requirement already satisfied: hdfs<3.0.0,>=2.1.0 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]->-r requirements.txt (line 2)) (2.7.3)\n",
            "Requirement already satisfied: httplib2<0.23.0,>=0.8 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]->-r requirements.txt (line 2)) (0.22.0)\n",
            "Requirement already satisfied: js2py<1,>=0.74 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]->-r requirements.txt (line 2)) (0.74)\n",
            "Requirement already satisfied: jsonschema<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]->-r requirements.txt (line 2)) (4.19.2)\n",
            "Requirement already satisfied: jsonpickle<4.0.0,>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]->-r requirements.txt (line 2)) (3.0.3)\n",
            "Requirement already satisfied: objsize<0.8.0,>=0.6.1 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]->-r requirements.txt (line 2)) (0.7.0)\n",
            "Requirement already satisfied: packaging>=22.0 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]->-r requirements.txt (line 2)) (24.0)\n",
            "Requirement already satisfied: pymongo<5.0.0,>=3.8.0 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]->-r requirements.txt (line 2)) (4.6.3)\n",
            "Requirement already satisfied: proto-plus<2,>=1.7.1 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]->-r requirements.txt (line 2)) (1.23.0)\n",
            "Requirement already satisfied: protobuf!=4.0.*,!=4.21.*,!=4.22.0,!=4.23.*,!=4.24.*,<4.26.0,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]->-r requirements.txt (line 2)) (3.20.3)\n",
            "Requirement already satisfied: pydot<2,>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]->-r requirements.txt (line 2)) (1.4.2)\n",
            "Requirement already satisfied: python-dateutil<3,>=2.8.0 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]->-r requirements.txt (line 2)) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2018.3 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]->-r requirements.txt (line 2)) (2023.4)\n",
            "Requirement already satisfied: regex>=2020.6.8 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]->-r requirements.txt (line 2)) (2023.12.25)\n",
            "Requirement already satisfied: typing-extensions>=3.7.0 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]->-r requirements.txt (line 2)) (4.10.0)\n",
            "Requirement already satisfied: zstandard<1,>=0.18.0 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]->-r requirements.txt (line 2)) (0.22.0)\n",
            "Requirement already satisfied: pyarrow<15.0.0,>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]->-r requirements.txt (line 2)) (14.0.2)\n",
            "Requirement already satisfied: pyarrow-hotfix<1 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]->-r requirements.txt (line 2)) (0.6)\n",
            "Requirement already satisfied: cachetools<6,>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]->-r requirements.txt (line 2)) (5.3.3)\n",
            "Requirement already satisfied: google-api-core<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]->-r requirements.txt (line 2)) (2.18.0)\n",
            "Requirement already satisfied: google-apitools<0.5.32,>=0.5.31 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]->-r requirements.txt (line 2)) (0.5.31)\n",
            "Requirement already satisfied: google-auth-httplib2<0.2.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]->-r requirements.txt (line 2)) (0.1.1)\n",
            "Requirement already satisfied: google-cloud-datastore<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]->-r requirements.txt (line 2)) (2.15.2)\n",
            "Requirement already satisfied: google-cloud-pubsub<3,>=2.1.0 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]->-r requirements.txt (line 2)) (2.21.1)\n",
            "Requirement already satisfied: google-cloud-pubsublite<2,>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]->-r requirements.txt (line 2)) (1.9.0)\n",
            "Requirement already satisfied: google-cloud-storage<3,>=2.14.0 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]->-r requirements.txt (line 2)) (2.16.0)\n",
            "Requirement already satisfied: google-cloud-bigquery<4,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]->-r requirements.txt (line 2)) (3.12.0)\n",
            "Requirement already satisfied: google-cloud-bigquery-storage<3,>=2.6.3 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]->-r requirements.txt (line 2)) (2.24.0)\n",
            "Requirement already satisfied: google-cloud-core<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]->-r requirements.txt (line 2)) (2.3.3)\n",
            "Requirement already satisfied: google-cloud-bigtable<3,>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]->-r requirements.txt (line 2)) (2.23.0)\n",
            "Requirement already satisfied: google-cloud-spanner<4,>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]->-r requirements.txt (line 2)) (3.44.0)\n",
            "Requirement already satisfied: google-cloud-dlp<4,>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]->-r requirements.txt (line 2)) (3.16.0)\n",
            "Requirement already satisfied: google-cloud-language<3,>=2.0 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]->-r requirements.txt (line 2)) (2.13.3)\n",
            "Requirement already satisfied: google-cloud-videointelligence<3,>=2.0 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]->-r requirements.txt (line 2)) (2.13.3)\n",
            "Requirement already satisfied: google-cloud-vision<4,>=2 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]->-r requirements.txt (line 2)) (3.7.2)\n",
            "Requirement already satisfied: google-cloud-recommendations-ai<0.11.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]->-r requirements.txt (line 2)) (0.10.10)\n",
            "Requirement already satisfied: google-cloud-aiplatform<2.0,>=1.26.0 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]->-r requirements.txt (line 2)) (1.46.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->-r requirements.txt (line 3)) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->-r requirements.txt (line 3)) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow->-r requirements.txt (line 3)) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow->-r requirements.txt (line 3)) (0.5.4)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow->-r requirements.txt (line 3)) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->-r requirements.txt (line 3)) (3.9.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->-r requirements.txt (line 3)) (18.1.1)\n",
            "Requirement already satisfied: ml-dtypes~=0.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->-r requirements.txt (line 3)) (0.2.0)\n",
            "INFO: pip is looking at multiple versions of tensorflow to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting tensorflow (from -r requirements.txt (line 3))\n",
            "  Using cached tensorflow-2.16.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (589.8 MB)\n",
            "Collecting h5py>=3.10.0 (from tensorflow->-r requirements.txt (line 3))\n",
            "  Using cached h5py-3.10.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.8 MB)\n",
            "Collecting ml-dtypes~=0.3.1 (from tensorflow->-r requirements.txt (line 3))\n",
            "  Using cached ml_dtypes-0.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.2 MB)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow->-r requirements.txt (line 3)) (3.3.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow->-r requirements.txt (line 3)) (67.7.2)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->-r requirements.txt (line 3)) (2.4.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->-r requirements.txt (line 3)) (1.14.1)\n",
            "Collecting tensorboard<2.17,>=2.16 (from tensorflow->-r requirements.txt (line 3))\n",
            "  Using cached tensorboard-2.16.2-py3-none-any.whl (5.5 MB)\n",
            "Collecting keras>=3.0.0 (from tensorflow->-r requirements.txt (line 3))\n",
            "  Using cached keras-3.1.1-py3-none-any.whl (1.1 MB)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow->-r requirements.txt (line 3)) (0.36.0)\n",
            "Collecting tensorflow (from -r requirements.txt (line 3))\n",
            "  Using cached tensorflow-2.15.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (475.2 MB)\n",
            "  Using cached tensorflow-2.15.0.post1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (475.2 MB)\n",
            "  Using cached tensorflow-2.14.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (489.9 MB)\n",
            "  Using cached tensorflow-2.14.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (489.8 MB)\n",
            "  Using cached tensorflow-2.13.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (479.7 MB)\n",
            "Collecting gast<=0.4.0,>=0.2.1 (from tensorflow->-r requirements.txt (line 3))\n",
            "  Using cached gast-0.4.0-py3-none-any.whl (9.8 kB)\n",
            "Collecting keras<2.14,>=2.13.1 (from tensorflow->-r requirements.txt (line 3))\n",
            "  Using cached keras-2.13.1-py3-none-any.whl (1.7 MB)\n",
            "Collecting tensorflow (from -r requirements.txt (line 3))\n",
            "  Using cached tensorflow-2.13.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (524.1 MB)\n",
            "INFO: pip is looking at multiple versions of tensorflow to determine which version is compatible with other requirements. This could take a while.\n",
            "  Using cached tensorflow-2.12.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (585.9 MB)\n",
            "Requirement already satisfied: jax>=0.3.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow->-r requirements.txt (line 3)) (0.4.23)\n",
            "Collecting keras<2.13,>=2.12.0 (from tensorflow->-r requirements.txt (line 3))\n",
            "  Using cached keras-2.12.0-py2.py3-none-any.whl (1.7 MB)\n",
            "Collecting tensorflow (from -r requirements.txt (line 3))\n",
            "  Using cached tensorflow-2.12.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (585.9 MB)\n",
            "  Using cached tensorflow-2.11.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (588.3 MB)\n",
            "Collecting keras<2.12,>=2.11.0 (from tensorflow->-r requirements.txt (line 3))\n",
            "  Using cached keras-2.11.0-py2.py3-none-any.whl (1.7 MB)\n",
            "Collecting tensorflow (from -r requirements.txt (line 3))\n",
            "  Using cached tensorflow-2.11.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (588.3 MB)\n",
            "  Using cached tensorflow-2.10.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (578.1 MB)\n",
            "Collecting keras<2.11,>=2.10.0 (from tensorflow->-r requirements.txt (line 3))\n",
            "  Using cached keras-2.10.0-py2.py3-none-any.whl (1.7 MB)\n",
            "Collecting keras-preprocessing>=1.1.1 (from tensorflow->-r requirements.txt (line 3))\n",
            "  Using cached Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
            "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
            "Collecting tensorflow (from -r requirements.txt (line 3))\n",
            "  Using cached tensorflow-2.10.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (578.0 MB)\n",
            "  Using cached tensorflow-2.9.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (511.8 MB)\n",
            "Collecting flatbuffers<2,>=1.12 (from tensorflow->-r requirements.txt (line 3))\n",
            "  Using cached flatbuffers-1.12-py2.py3-none-any.whl (15 kB)\n",
            "Collecting keras<2.10.0,>=2.9.0rc0 (from tensorflow->-r requirements.txt (line 3))\n",
            "  Using cached keras-2.9.0-py2.py3-none-any.whl (1.6 MB)\n",
            "Collecting tensorflow (from -r requirements.txt (line 3))\n",
            "  Using cached tensorflow-2.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (511.8 MB)\n",
            "  Using cached tensorflow-2.9.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (511.7 MB)\n",
            "  Using cached tensorflow-2.9.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (511.7 MB)\n",
            "Collecting tensorboard<2.10,>=2.9 (from tensorflow->-r requirements.txt (line 3))\n",
            "  Using cached tensorboard-2.9.1-py3-none-any.whl (5.8 MB)\n",
            "Collecting tensorflow-estimator<2.10.0,>=2.9.0rc0 (from tensorflow->-r requirements.txt (line 3))\n",
            "  Using cached tensorflow_estimator-2.9.0-py2.py3-none-any.whl (438 kB)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow->-r requirements.txt (line 3)) (0.43.0)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core<3,>=2.0.0->apache-beam[gcp]->-r requirements.txt (line 2)) (1.63.0)\n",
            "Requirement already satisfied: oauth2client>=1.4.12 in /usr/local/lib/python3.10/dist-packages (from google-apitools<0.5.32,>=0.5.31->apache-beam[gcp]->-r requirements.txt (line 2)) (4.1.3)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.2->gcsfs==0.7.1->-r requirements.txt (line 6)) (0.4.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.2->gcsfs==0.7.1->-r requirements.txt (line 6)) (4.9)\n",
            "Requirement already satisfied: google-cloud-resource-manager<3.0.0dev,>=1.3.3 in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform<2.0,>=1.26.0->apache-beam[gcp]->-r requirements.txt (line 2)) (1.12.3)\n",
            "Requirement already satisfied: shapely<3.0.0dev in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform<2.0,>=1.26.0->apache-beam[gcp]->-r requirements.txt (line 2)) (2.0.3)\n",
            "Requirement already satisfied: pydantic<3 in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform<2.0,>=1.26.0->apache-beam[gcp]->-r requirements.txt (line 2)) (2.6.4)\n",
            "Requirement already satisfied: docstring-parser<1 in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform<2.0,>=1.26.0->apache-beam[gcp]->-r requirements.txt (line 2)) (0.16)\n",
            "Requirement already satisfied: google-resumable-media<3.0dev,>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-bigquery<4,>=2.0.0->apache-beam[gcp]->-r requirements.txt (line 2)) (2.7.0)\n",
            "Requirement already satisfied: grpc-google-iam-v1<1.0.0dev,>=0.12.4 in /usr/local/lib/python3.10/dist-packages (from google-cloud-bigtable<3,>=2.19.0->apache-beam[gcp]->-r requirements.txt (line 2)) (0.13.0)\n",
            "Requirement already satisfied: grpcio-status>=1.33.2 in /usr/local/lib/python3.10/dist-packages (from google-cloud-pubsub<3,>=2.1.0->apache-beam[gcp]->-r requirements.txt (line 2)) (1.48.2)\n",
            "Requirement already satisfied: overrides<8.0.0,>=6.0.1 in /usr/local/lib/python3.10/dist-packages (from google-cloud-pubsublite<2,>=1.2.0->apache-beam[gcp]->-r requirements.txt (line 2)) (7.7.0)\n",
            "Requirement already satisfied: sqlparse>=0.4.4 in /usr/local/lib/python3.10/dist-packages (from google-cloud-spanner<4,>=3.0.0->apache-beam[gcp]->-r requirements.txt (line 2)) (0.4.4)\n",
            "Requirement already satisfied: deprecated>=1.2.14 in /usr/local/lib/python3.10/dist-packages (from google-cloud-spanner<4,>=3.0.0->apache-beam[gcp]->-r requirements.txt (line 2)) (1.2.14)\n",
            "Requirement already satisfied: grpc-interceptor>=0.15.4 in /usr/local/lib/python3.10/dist-packages (from google-cloud-spanner<4,>=3.0.0->apache-beam[gcp]->-r requirements.txt (line 2)) (0.15.4)\n",
            "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-storage<3,>=2.14.0->apache-beam[gcp]->-r requirements.txt (line 2)) (1.5.0)\n",
            "Requirement already satisfied: docopt in /usr/local/lib/python3.10/dist-packages (from hdfs<3.0.0,>=2.1.0->apache-beam[gcp]->-r requirements.txt (line 2)) (0.6.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /usr/local/lib/python3.10/dist-packages (from httplib2<0.23.0,>=0.8->apache-beam[gcp]->-r requirements.txt (line 2)) (3.1.2)\n",
            "Requirement already satisfied: tzlocal>=1.2 in /usr/local/lib/python3.10/dist-packages (from js2py<1,>=0.74->apache-beam[gcp]->-r requirements.txt (line 2)) (5.2)\n",
            "Requirement already satisfied: pyjsparser>=2.5.1 in /usr/local/lib/python3.10/dist-packages (from js2py<1,>=0.74->apache-beam[gcp]->-r requirements.txt (line 2)) (2.7.1)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema<5.0.0,>=4.0.0->apache-beam[gcp]->-r requirements.txt (line 2)) (23.2.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema<5.0.0,>=4.0.0->apache-beam[gcp]->-r requirements.txt (line 2)) (2023.12.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema<5.0.0,>=4.0.0->apache-beam[gcp]->-r requirements.txt (line 2)) (0.34.0)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema<5.0.0,>=4.0.0->apache-beam[gcp]->-r requirements.txt (line 2)) (0.18.0)\n",
            "Requirement already satisfied: dnspython<3.0.0,>=1.16.0 in /usr/local/lib/python3.10/dist-packages (from pymongo<5.0.0,>=3.8.0->apache-beam[gcp]->-r requirements.txt (line 2)) (2.6.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->gcsfs==0.7.1->-r requirements.txt (line 6)) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->gcsfs==0.7.1->-r requirements.txt (line 6)) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->gcsfs==0.7.1->-r requirements.txt (line 6)) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->gcsfs==0.7.1->-r requirements.txt (line 6)) (2024.2.2)\n",
            "INFO: pip is looking at multiple versions of scipy to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting scipy>=0.18.1 (from gensim==3.6.0->-r requirements.txt (line 4))\n",
            "  Using cached scipy-1.13.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38.6 MB)\n",
            "  Using cached scipy-1.12.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38.4 MB)\n",
            "  Using cached scipy-1.11.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (36.4 MB)\n",
            "  Using cached scipy-1.11.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (36.3 MB)\n",
            "  Using cached scipy-1.11.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (36.3 MB)\n",
            "  Using cached scipy-1.10.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (34.4 MB)\n",
            "Collecting google-auth-oauthlib (from gcsfs==0.7.1->-r requirements.txt (line 6))\n",
            "  Using cached google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.10,>=2.9->tensorflow->-r requirements.txt (line 3)) (3.6)\n",
            "INFO: pip is looking at multiple versions of tensorboard to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting tensorboard<2.10,>=2.9 (from tensorflow->-r requirements.txt (line 3))\n",
            "  Using cached tensorboard-2.9.0-py3-none-any.whl (5.8 MB)\n",
            "Collecting tensorboard-data-server<0.7.0,>=0.6.0 (from tensorboard<2.10,>=2.9->tensorflow->-r requirements.txt (line 3))\n",
            "  Using cached tensorboard_data_server-0.6.1-py3-none-manylinux2010_x86_64.whl (4.9 MB)\n",
            "Collecting tensorboard-plugin-wit>=1.6.0 (from tensorboard<2.10,>=2.9->tensorflow->-r requirements.txt (line 3))\n",
            "  Using cached tensorboard_plugin_wit-1.8.1-py3-none-any.whl (781 kB)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.10,>=2.9->tensorflow->-r requirements.txt (line 3)) (3.0.2)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib->gcsfs==0.7.1->-r requirements.txt (line 6)) (1.3.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->gcsfs==0.7.1->-r requirements.txt (line 6)) (1.3.1)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->gcsfs==0.7.1->-r requirements.txt (line 6)) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->gcsfs==0.7.1->-r requirements.txt (line 6)) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->gcsfs==0.7.1->-r requirements.txt (line 6)) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->gcsfs==0.7.1->-r requirements.txt (line 6)) (4.0.3)\n",
            "Requirement already satisfied: pyasn1>=0.1.7 in /usr/local/lib/python3.10/dist-packages (from oauth2client>=1.4.12->google-apitools<0.5.32,>=0.5.31->apache-beam[gcp]->-r requirements.txt (line 2)) (0.6.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3->google-cloud-aiplatform<2.0,>=1.26.0->apache-beam[gcp]->-r requirements.txt (line 2)) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.16.3 in /usr/local/lib/python3.10/dist-packages (from pydantic<3->google-cloud-aiplatform<2.0,>=1.26.0->apache-beam[gcp]->-r requirements.txt (line 2)) (2.16.3)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib->gcsfs==0.7.1->-r requirements.txt (line 6)) (3.2.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.10,>=2.9->tensorflow->-r requirements.txt (line 3)) (2.1.5)\n",
            "Building wheels for collected packages: numpy\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mBuilding wheel for numpy \u001b[0m\u001b[1;32m(\u001b[0m\u001b[32mpyproject.toml\u001b[0m\u001b[1;32m)\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Building wheel for numpy (pyproject.toml) ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[31m  ERROR: Failed building wheel for numpy\u001b[0m\u001b[31m\n",
            "\u001b[0mFailed to build numpy\n",
            "\u001b[31mERROR: Could not build wheels for numpy, which is required to install pyproject.toml-based projects\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "! pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {
        "id": "jRCMqESUnUcq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8498b9d6-4535-475d-d55b-03e90221dd9c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: apache-beam[gcp] in /usr/local/lib/python3.10/dist-packages (2.55.0)\n",
            "Requirement already satisfied: crcmod<2.0,>=1.7 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]) (1.7)\n",
            "Requirement already satisfied: orjson<4,>=3.9.7 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]) (3.10.0)\n",
            "Requirement already satisfied: dill<0.3.2,>=0.3.1.1 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]) (0.3.1.1)\n",
            "Requirement already satisfied: cloudpickle~=2.2.1 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]) (2.2.1)\n",
            "Requirement already satisfied: fastavro<2,>=0.23.6 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]) (1.9.4)\n",
            "Requirement already satisfied: fasteners<1.0,>=0.3 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]) (0.19)\n",
            "Requirement already satisfied: grpcio!=1.48.0,<2,>=1.33.1 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]) (1.62.1)\n",
            "Requirement already satisfied: hdfs<3.0.0,>=2.1.0 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]) (2.7.3)\n",
            "Requirement already satisfied: httplib2<0.23.0,>=0.8 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]) (0.22.0)\n",
            "Requirement already satisfied: js2py<1,>=0.74 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]) (0.74)\n",
            "Requirement already satisfied: jsonschema<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]) (4.19.2)\n",
            "Requirement already satisfied: jsonpickle<4.0.0,>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]) (3.0.3)\n",
            "Requirement already satisfied: numpy<1.27.0,>=1.14.3 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]) (1.25.2)\n",
            "Requirement already satisfied: objsize<0.8.0,>=0.6.1 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]) (0.7.0)\n",
            "Requirement already satisfied: packaging>=22.0 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]) (24.0)\n",
            "Requirement already satisfied: pymongo<5.0.0,>=3.8.0 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]) (4.6.3)\n",
            "Requirement already satisfied: proto-plus<2,>=1.7.1 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]) (1.23.0)\n",
            "Requirement already satisfied: protobuf!=4.0.*,!=4.21.*,!=4.22.0,!=4.23.*,!=4.24.*,<4.26.0,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]) (3.20.3)\n",
            "Requirement already satisfied: pydot<2,>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]) (1.4.2)\n",
            "Requirement already satisfied: python-dateutil<3,>=2.8.0 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2018.3 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]) (2023.4)\n",
            "Requirement already satisfied: regex>=2020.6.8 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]) (2023.12.25)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.24.0 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]) (2.31.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.0 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]) (4.10.0)\n",
            "Requirement already satisfied: zstandard<1,>=0.18.0 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]) (0.22.0)\n",
            "Requirement already satisfied: pyarrow<15.0.0,>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]) (14.0.2)\n",
            "Requirement already satisfied: pyarrow-hotfix<1 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]) (0.6)\n",
            "Requirement already satisfied: cachetools<6,>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]) (5.3.3)\n",
            "Requirement already satisfied: google-api-core<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]) (2.18.0)\n",
            "Requirement already satisfied: google-apitools<0.5.32,>=0.5.31 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]) (0.5.31)\n",
            "Requirement already satisfied: google-auth<3,>=1.18.0 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]) (2.27.0)\n",
            "Requirement already satisfied: google-auth-httplib2<0.2.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]) (0.1.1)\n",
            "Requirement already satisfied: google-cloud-datastore<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]) (2.15.2)\n",
            "Requirement already satisfied: google-cloud-pubsub<3,>=2.1.0 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]) (2.21.1)\n",
            "Requirement already satisfied: google-cloud-pubsublite<2,>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]) (1.9.0)\n",
            "Requirement already satisfied: google-cloud-storage<3,>=2.14.0 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]) (2.16.0)\n",
            "Requirement already satisfied: google-cloud-bigquery<4,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]) (3.12.0)\n",
            "Requirement already satisfied: google-cloud-bigquery-storage<3,>=2.6.3 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]) (2.24.0)\n",
            "Requirement already satisfied: google-cloud-core<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]) (2.3.3)\n",
            "Requirement already satisfied: google-cloud-bigtable<3,>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]) (2.23.0)\n",
            "Requirement already satisfied: google-cloud-spanner<4,>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]) (3.44.0)\n",
            "Requirement already satisfied: google-cloud-dlp<4,>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]) (3.16.0)\n",
            "Requirement already satisfied: google-cloud-language<3,>=2.0 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]) (2.13.3)\n",
            "Requirement already satisfied: google-cloud-videointelligence<3,>=2.0 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]) (2.13.3)\n",
            "Requirement already satisfied: google-cloud-vision<4,>=2 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]) (3.7.2)\n",
            "Requirement already satisfied: google-cloud-recommendations-ai<0.11.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]) (0.10.10)\n",
            "Requirement already satisfied: google-cloud-aiplatform<2.0,>=1.26.0 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]) (1.46.0)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core<3,>=2.0.0->apache-beam[gcp]) (1.63.0)\n",
            "Requirement already satisfied: oauth2client>=1.4.12 in /usr/local/lib/python3.10/dist-packages (from google-apitools<0.5.32,>=0.5.31->apache-beam[gcp]) (4.1.3)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from google-apitools<0.5.32,>=0.5.31->apache-beam[gcp]) (1.16.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.18.0->apache-beam[gcp]) (0.4.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.18.0->apache-beam[gcp]) (4.9)\n",
            "Requirement already satisfied: google-cloud-resource-manager<3.0.0dev,>=1.3.3 in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform<2.0,>=1.26.0->apache-beam[gcp]) (1.12.3)\n",
            "Requirement already satisfied: shapely<3.0.0dev in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform<2.0,>=1.26.0->apache-beam[gcp]) (2.0.3)\n",
            "Requirement already satisfied: pydantic<3 in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform<2.0,>=1.26.0->apache-beam[gcp]) (2.6.4)\n",
            "Requirement already satisfied: docstring-parser<1 in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform<2.0,>=1.26.0->apache-beam[gcp]) (0.16)\n",
            "Requirement already satisfied: google-resumable-media<3.0dev,>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-bigquery<4,>=2.0.0->apache-beam[gcp]) (2.7.0)\n",
            "Requirement already satisfied: grpc-google-iam-v1<1.0.0dev,>=0.12.4 in /usr/local/lib/python3.10/dist-packages (from google-cloud-bigtable<3,>=2.19.0->apache-beam[gcp]) (0.13.0)\n",
            "Requirement already satisfied: grpcio-status>=1.33.2 in /usr/local/lib/python3.10/dist-packages (from google-cloud-pubsub<3,>=2.1.0->apache-beam[gcp]) (1.48.2)\n",
            "Requirement already satisfied: overrides<8.0.0,>=6.0.1 in /usr/local/lib/python3.10/dist-packages (from google-cloud-pubsublite<2,>=1.2.0->apache-beam[gcp]) (7.7.0)\n",
            "Requirement already satisfied: sqlparse>=0.4.4 in /usr/local/lib/python3.10/dist-packages (from google-cloud-spanner<4,>=3.0.0->apache-beam[gcp]) (0.4.4)\n",
            "Requirement already satisfied: deprecated>=1.2.14 in /usr/local/lib/python3.10/dist-packages (from google-cloud-spanner<4,>=3.0.0->apache-beam[gcp]) (1.2.14)\n",
            "Requirement already satisfied: grpc-interceptor>=0.15.4 in /usr/local/lib/python3.10/dist-packages (from google-cloud-spanner<4,>=3.0.0->apache-beam[gcp]) (0.15.4)\n",
            "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-storage<3,>=2.14.0->apache-beam[gcp]) (1.5.0)\n",
            "Requirement already satisfied: docopt in /usr/local/lib/python3.10/dist-packages (from hdfs<3.0.0,>=2.1.0->apache-beam[gcp]) (0.6.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /usr/local/lib/python3.10/dist-packages (from httplib2<0.23.0,>=0.8->apache-beam[gcp]) (3.1.2)\n",
            "Requirement already satisfied: tzlocal>=1.2 in /usr/local/lib/python3.10/dist-packages (from js2py<1,>=0.74->apache-beam[gcp]) (5.2)\n",
            "Requirement already satisfied: pyjsparser>=2.5.1 in /usr/local/lib/python3.10/dist-packages (from js2py<1,>=0.74->apache-beam[gcp]) (2.7.1)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema<5.0.0,>=4.0.0->apache-beam[gcp]) (23.2.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema<5.0.0,>=4.0.0->apache-beam[gcp]) (2023.12.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema<5.0.0,>=4.0.0->apache-beam[gcp]) (0.34.0)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema<5.0.0,>=4.0.0->apache-beam[gcp]) (0.18.0)\n",
            "Requirement already satisfied: dnspython<3.0.0,>=1.16.0 in /usr/local/lib/python3.10/dist-packages (from pymongo<5.0.0,>=3.8.0->apache-beam[gcp]) (2.6.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.24.0->apache-beam[gcp]) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.24.0->apache-beam[gcp]) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.24.0->apache-beam[gcp]) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.24.0->apache-beam[gcp]) (2024.2.2)\n",
            "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.10/dist-packages (from deprecated>=1.2.14->google-cloud-spanner<4,>=3.0.0->apache-beam[gcp]) (1.14.1)\n",
            "Requirement already satisfied: pyasn1>=0.1.7 in /usr/local/lib/python3.10/dist-packages (from oauth2client>=1.4.12->google-apitools<0.5.32,>=0.5.31->apache-beam[gcp]) (0.6.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3->google-cloud-aiplatform<2.0,>=1.26.0->apache-beam[gcp]) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.16.3 in /usr/local/lib/python3.10/dist-packages (from pydantic<3->google-cloud-aiplatform<2.0,>=1.26.0->apache-beam[gcp]) (2.16.3)\n"
          ]
        }
      ],
      "source": [
        "! pip install apache-beam[gcp]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k0DDu27S3CJv"
      },
      "source": [
        "## Primer ejercicio\n",
        "\n",
        "Desarrollar un pipeline de preprocesamiento utilizando Apache Beam para generar datos de train, eval y test para los datos proporcionados anteriormente. Requisitos:\n",
        "\n",
        "- Proporcionar dos modos de ejecución: `train` y `test`\n",
        "- Soportar ejecuciones en local con `DirectRunner` y ejecuciones en Dataflow usando `DataFlowRunner`."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "WORK_DIR = os.getcwd()"
      ],
      "metadata": {
        "id": "mnncrDhu_9KX"
      },
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "WORK_DIR"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "SHGeUSRn__op",
        "outputId": "238e121b-4497-4c76-bdad-4c1be8ceea25"
      },
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/batch/twitter-sentiment-batch'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 91
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import json\n",
        "import argparse\n",
        "import logging\n",
        "import random\n",
        "import csv\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import SnowballStemmer\n",
        "import apache_beam as beam\n",
        "from apache_beam.io import ReadFromText, WriteToText\n",
        "from apache_beam.options.pipeline_options import PipelineOptions, SetupOptions, DirectOptions\n",
        "from apache_beam.coders.coders import Coder"
      ],
      "metadata": {
        "id": "lShXneo0-rBZ"
      },
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile preprocess.py\n",
        "\n",
        "nltk.download(\"stopwords\")\n",
        "\n",
        "# CLEANING\n",
        "STOP_WORDS = stopwords.words(\"english\")\n",
        "STEMMER = SnowballStemmer(\"english\")\n",
        "TEXT_CLEANING_RE = \"@\\S+|https?:\\S+|http?:\\S|[^A-Za-z0-9]+\"\n",
        "\n",
        "\n",
        "# class ExtractColumnsDoFn(beam.DoFn):\n",
        "#     def process(self, element):\n",
        "#         # space removal\n",
        "#         element_split = list(csv.reader(element, delimiter=\",\"))\n",
        "#         element_split = [x for x in element_split if x != [\"\", \"\"]]\n",
        "#         # text, sentiment\n",
        "#         yield element_split[5][-1], element_split[0][-1]\n",
        "\n",
        "class ExtractColumnsDoFn(beam.DoFn):\n",
        "    def process(self, element):\n",
        "        #se carga el archivo json\n",
        "        comentarios = json.loads(element)\n",
        "        #Selecciono las columnas que contienen los comentarios y la etiqueta 1 o 0 (si es troll o no es troll)\n",
        "        texto_comentario = comentarios[\"content\"]\n",
        "        troll = comentarios[\"annotation\"][\"label\"][0]\n",
        "        data_troll = int(troll)\n",
        "        yield texto_comentario, data_troll\n",
        "\n",
        "class PreprocessColumnsTrainFn(beam.DoFn):\n",
        "    def process_sentiment(self, sentiment):\n",
        "        sentiment = int(sentiment)\n",
        "        if sentiment == 1:\n",
        "            return \"TROLL\"\n",
        "        else:\n",
        "            return \"NO_TROLL\"\n",
        "\n",
        "    def process_text(self, text):\n",
        "        # Remove link,user and special characters\n",
        "        stem = False\n",
        "        text = re.sub(TEXT_CLEANING_RE, \" \", str(text).lower()).strip()\n",
        "        tokens = []\n",
        "        for token in text.split():\n",
        "            if token not in STOP_WORDS:\n",
        "                if stem:\n",
        "                    tokens.append(STEMMER.stem(token))\n",
        "                else:\n",
        "                    tokens.append(token)\n",
        "        return \" \".join(tokens)\n",
        "\n",
        "    def process(self, element):\n",
        "        processed_text = self.process_text(element[0])\n",
        "        processed_sentiment = self.process_sentiment(element[1])\n",
        "        yield f\"{processed_text}, {processed_sentiment}\"\n",
        "\n",
        "\n",
        "class CustomCoder(Coder):\n",
        "    \"\"\"A custom coder used for reading and writing strings\"\"\"\n",
        "\n",
        "    def __init__(self, encoding: str):\n",
        "        # latin-1\n",
        "        # iso-8859-1\n",
        "        self.enconding = encoding\n",
        "\n",
        "    def encode(self, value):\n",
        "        return value.encode(self.enconding)\n",
        "\n",
        "    def decode(self, value):\n",
        "        return value.decode(self.enconding)\n",
        "\n",
        "    def is_deterministic(self):\n",
        "        return True\n",
        "\n",
        "\n",
        "def run(argv=None, save_main_session=True):\n",
        "\n",
        "    \"\"\"Main entry point; defines and runs the wordcount pipeline.\"\"\"\n",
        "\n",
        "    parser = argparse.ArgumentParser()\n",
        "\n",
        "    parser.add_argument(\n",
        "        \"--work-dir\", dest=\"work_dir\", required=True, help=\"Working directory\",\n",
        "    )\n",
        "\n",
        "    parser.add_argument(\n",
        "        \"--input\", dest=\"input\", required=True, help=\"Input dataset in work dir\",\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--output\",\n",
        "        dest=\"output\",\n",
        "        required=True,\n",
        "        help=\"Output path to store transformed data in work dir\",\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--mode\",\n",
        "        dest=\"mode\",\n",
        "        required=True,\n",
        "        choices=[\"train\", \"test\"],\n",
        "        help=\"Type of output to store transformed data\",\n",
        "    )\n",
        "\n",
        "    known_args, pipeline_args = parser.parse_known_args(argv)\n",
        "\n",
        "    # We use the save_main_session option because one or more DoFn's in this\n",
        "    # workflow rely on global context (e.g., a module imported at module level).\n",
        "    pipeline_options = PipelineOptions(pipeline_args)\n",
        "    pipeline_options.view_as(SetupOptions).save_main_session = save_main_session\n",
        "    pipeline_options.view_as(DirectOptions).direct_num_workers = 0\n",
        "\n",
        "    # The pipeline will be run on exiting the with block.\n",
        "    with beam.Pipeline(options=pipeline_options) as p:\n",
        "\n",
        "        # Read the text file[pattern] into a PCollection.\n",
        "        raw_data = p | \"ReadTwitterData\" >> ReadFromText(\n",
        "            known_args.input, coder=CustomCoder(\"latin-1\")\n",
        "        )\n",
        "\n",
        "        if known_args.mode == \"train\":\n",
        "\n",
        "            transformed_data = (\n",
        "                raw_data\n",
        "                | \"ExtractColumns\" >> beam.ParDo(ExtractColumnsDoFn())\n",
        "                | \"Preprocess\" >> beam.ParDo(PreprocessColumnsTrainFn())\n",
        "            )\n",
        "\n",
        "            eval_percent = 20\n",
        "            assert 0 < eval_percent < 100, \"eval_percent must in the range (0-100)\"\n",
        "            train_dataset, eval_dataset = (\n",
        "                transformed_data\n",
        "                | \"Split dataset\"\n",
        "                >> beam.Partition(\n",
        "                    lambda elem, _: int(random.uniform(0, 100) < eval_percent), 2\n",
        "                )\n",
        "            )\n",
        "\n",
        "            train_dataset | \"TrainWriteToCSV\" >> WriteToText(\n",
        "                os.path.join(known_args.output, \"train\", \"part\")\n",
        "            )\n",
        "            eval_dataset | \"EvalWriteToCSV\" >> WriteToText(\n",
        "                os.path.join(known_args.output, \"eval\", \"part\")\n",
        "            )\n",
        "\n",
        "        else:\n",
        "            transformed_data = (\n",
        "                raw_data\n",
        "                | \"ExtractColumns\" >> beam.ParDo(ExtractColumnsDoFn())\n",
        "                | \"Preprocess\" >> beam.Map(lambda x: f'\"{x[0]}\"')\n",
        "            )\n",
        "\n",
        "            transformed_data | \"TestWriteToCSV\" >> WriteToText(\n",
        "                os.path.join(known_args.output, \"test\", \"part\")\n",
        "            )\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    logging.getLogger().setLevel(logging.INFO)\n",
        "    run()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l4STVoCd-w5s",
        "outputId": "7b4ce69c-62e7-4246-d88d-7d5b78546ecc"
      },
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting preprocess.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qawuKC0k4B0e"
      },
      "source": [
        "Se proporciona un fichero `setup.py` necesario para ejecutar en DataFlow. Modificar la variable `REQUIRED_PACKAGES` con las dependencias que se hayan usado en el `requirements.txt`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "metadata": {
        "id": "n1MQvWsk_mVX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "93a658ba-b5b8-444e-bfd0-c16c279bb49a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting setup.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile setup.py\n",
        "\n",
        "import setuptools\n",
        "\n",
        "REQUIRED_PACKAGES = [\n",
        "  \"apache-beam[gcp]==2.24.0\",\n",
        "  \"tensorflow==2.8.0\",\n",
        "  \"gensim==3.6.0\",\n",
        "  \"fsspec==0.8.4\",\n",
        "  \"gcsfs==0.7.1\",\n",
        "  \"numpy==1.20.0\"\n",
        "]\n",
        "\n",
        "setuptools.setup(\n",
        "    name=\"twitchstreaming\",\n",
        "    version=\"0.0.1\",\n",
        "    install_requires=REQUIRED_PACKAGES,\n",
        "    packages=setuptools.find_packages(),\n",
        "    include_package_data=True,\n",
        "    description=\"Troll detection\",\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hxYFYMy-o55h"
      },
      "source": [
        "Me creo una copia por si hubiera algún error en el procesamiento (buena práctica):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "metadata": {
        "id": "KlBCbvSQpBUK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d53be38e-7ea0-4541-9d41-4245b1108be9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1dTaKofC9ZcMWa5cVtGLDFkEnbc4hiPJr\n",
            "To: /content/batch/twitter-sentiment-batch/dataset-cybertrolls.json\n",
            "100% 2.76M/2.76M [00:00<00:00, 151MB/s]\n",
            "Copying file://dataset-cybertrolls.json...\n",
            "/ [1 files][  2.6 MiB/  2.6 MiB]                                                \n",
            "Operation completed over 1 objects/2.6 MiB.                                      \n"
          ]
        }
      ],
      "source": [
        "! gdown \"1dTaKofC9ZcMWa5cVtGLDFkEnbc4hiPJr\"\n",
        "! gsutil cp dataset-cybertrolls.json //$WORK_DIR/data.json\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jLDlz_fL4UJA"
      },
      "source": [
        "### Validación preprocess train en local\n",
        "Con el comando mostrado a continuación se valida la correcta generación de los datos de entrenamiento y validación en local."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "metadata": {
        "id": "vFpirg37C3bN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ed003b21-2e6e-4cc0-d133-f59ae29662f6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/content/batch/twitter-sentiment-batch/preprocess.py\", line 2, in <module>\n",
            "    nltk.download(\"stopwords\")\n",
            "NameError: name 'nltk' is not defined\n"
          ]
        }
      ],
      "source": [
        "! python3 preprocess.py \\\n",
        "  --work-dir $WORK_DIR \\\n",
        "  --runner DirectRunner \\\n",
        "  --input $WORK_DIR/data/training*.csv \\\n",
        "  --output $WORK_DIR/data/transformed_data \\\n",
        "  --mode train"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wkkG271a4qnA"
      },
      "source": [
        "### Validación preprocess test en local\n",
        "\n",
        "Con el comando mostrado a continuación se valida la correcta generación de los datos de test en local."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "metadata": {
        "id": "u4lkLgecLS3i",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "db176bcd-4894-4972-8285-387dc297db19"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/content/batch/twitter-sentiment-batch/preprocess.py\", line 2, in <module>\n",
            "    nltk.download(\"stopwords\")\n",
            "NameError: name 'nltk' is not defined\n"
          ]
        }
      ],
      "source": [
        "! python3 preprocess.py \\\n",
        "  --work-dir $WORK_DIR \\\n",
        "  --runner DirectRunner \\\n",
        "  --input $WORK_DIR/data/test*.csv \\\n",
        "  --output $WORK_DIR/data/transformed_data \\\n",
        "  --mode test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "geZM9Sbj45LK"
      },
      "source": [
        "## Segundo ejercicio\n",
        "\n",
        "Desarrollar una tarea de entrenamiento para los datos preprocesados. Requisitos:\n",
        "\n",
        "- Soportar ejecuciones en local usando el SDK de AI-Platform y ejecuciones en GCP con el mismo código."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OMUwXgm_5el-"
      },
      "source": [
        "Se crea el directorio donde trabajaremos:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "metadata": {
        "id": "HMi8dI1gLoIc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eec916da-6006-41ad-f00f-96616f189a59"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mkdir: cannot create directory ‘/content/batch/trainer’: File exists\n",
            "mkdir: cannot create directory ‘/content/batch/trainer/data’: File exists\n"
          ]
        }
      ],
      "source": [
        "%mkdir /content/batch/trainer\n",
        "%mkdir /content/batch/trainer/data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "metadata": {
        "id": "3dJyMXTuNPwo"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "if not os.path.exists('trainer'):\n",
        "    os.makedirs('trainer')\n",
        "\n",
        "version = \"0.1.0\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile trainer/task.py\n",
        "\n",
        "from __future__ import absolute_import\n",
        "\n",
        "import argparse\n",
        "import multiprocessing as mp\n",
        "import logging\n",
        "import tempfile\n",
        "import os\n",
        "\n",
        "import pickle\n",
        "import gensim\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import json\n",
        "\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import (\n",
        "    Dense,\n",
        "    Dropout,\n",
        "    Embedding,\n",
        "    LSTM,\n",
        ")\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "\n",
        "# WORD2VEC\n",
        "W2V_SIZE = 300\n",
        "W2V_WINDOW = 7\n",
        "# 32\n",
        "W2V_EPOCH = 5\n",
        "W2V_MIN_COUNT = 10\n",
        "\n",
        "# KERAS\n",
        "SEQUENCE_LENGTH = 300\n",
        "\n",
        "#Trolls\n",
        "POSITIVE = \"TROLL\"\n",
        "NEGATIVE = \"NO_TROLL\"\n",
        "#TROLL_THRESHOLDS = 0.5\n",
        "\n",
        "# SENTIMENT\n",
        "#POSITIVE = \"POSITIVE\"\n",
        "#NEGATIVE = \"NEGATIVE\"\n",
        "#NEUTRAL = \"NEUTRAL\"\n",
        "#SENTIMENT_THRESHOLDS = (0.4, 0.7)\n",
        "\n",
        "\n",
        "# EXPORT\n",
        "KERAS_MODEL = \"model.h5\"\n",
        "WORD2VEC_MODEL = \"model.w2v\"\n",
        "TOKENIZER_MODEL = \"tokenizer.pkl\"\n",
        "ENCODER_MODEL = \"encoder.pkl\"\n",
        "\n",
        "\n",
        "def generate_word2vec(train_df):\n",
        "    documents = [_text.split() for _text in train_df.text.values]\n",
        "    w2v_model = gensim.models.word2vec.Word2Vec(\n",
        "        size=W2V_SIZE,\n",
        "        window=W2V_WINDOW,\n",
        "        min_count=W2V_MIN_COUNT,\n",
        "        workers=mp.cpu_count(),\n",
        "    )\n",
        "    w2v_model.build_vocab(documents)\n",
        "\n",
        "    words = w2v_model.wv.vocab.keys()\n",
        "    vocab_size = len(words)\n",
        "    logging.info(f\"Vocab size: {vocab_size}\")\n",
        "    w2v_model.train(documents, total_examples=len(documents), epochs=W2V_EPOCH)\n",
        "\n",
        "    return w2v_model\n",
        "\n",
        "\n",
        "def generate_tokenizer(train_df):\n",
        "    tokenizer = Tokenizer()\n",
        "    tokenizer.fit_on_texts(train_df.text)\n",
        "    vocab_size = len(tokenizer.word_index) + 1\n",
        "    logging.info(f\"Total words: {vocab_size}\")\n",
        "    return tokenizer, vocab_size\n",
        "\n",
        "\n",
        "def generate_label_encoder(train_df):\n",
        "    encoder = LabelEncoder()\n",
        "    encoder.fit(train_df.sentiment.tolist())\n",
        "    return encoder\n",
        "\n",
        "\n",
        "def generate_embedding(word2vec_model, vocab_size, tokenizer):\n",
        "    embedding_matrix = np.zeros((vocab_size, W2V_SIZE))\n",
        "    for word, i in tokenizer.word_index.items():\n",
        "        if word in word2vec_model.wv:\n",
        "            embedding_matrix[i] = word2vec_model.wv[word]\n",
        "    return Embedding(\n",
        "        vocab_size,\n",
        "        W2V_SIZE,\n",
        "        weights=[embedding_matrix],\n",
        "        input_length=SEQUENCE_LENGTH,\n",
        "        trainable=False,\n",
        "    )\n",
        "\n",
        "\n",
        "def train_and_evaluate(\n",
        "    work_dir, train_df, eval_df, batch_size=1024, epochs=8, steps=1000\n",
        "):\n",
        "\n",
        "    \"\"\"\n",
        "    Trains and evaluates the estimator given.\n",
        "    The input functions are generated by the preprocessing function.\n",
        "    \"\"\"\n",
        "\n",
        "    model_dir = os.path.join(work_dir, \"model\")\n",
        "    if tf.io.gfile.exists(model_dir):\n",
        "        tf.io.gfile.rmtree(model_dir)\n",
        "    tf.io.gfile.mkdir(model_dir)\n",
        "\n",
        "    # Specify where to store our model\n",
        "    run_config = tf.estimator.RunConfig()\n",
        "    run_config = run_config.replace(model_dir=model_dir)\n",
        "\n",
        "    # This will give us a more granular visualization of the training\n",
        "    run_config = run_config.replace(save_summary_steps=10)\n",
        "\n",
        "    # Create Word2vec of training data\n",
        "    logging.info(\"---- Generating word2vec model ----\")\n",
        "    word2vec_model = generate_word2vec(train_df)\n",
        "\n",
        "    # Tokenize training data\n",
        "    logging.info(\"---- Generating tokenizer ----\")\n",
        "    tokenizer, vocab_size = generate_tokenizer(train_df)\n",
        "\n",
        "    logging.info(\"---- Tokenizing train data ----\")\n",
        "    x_train = pad_sequences(\n",
        "        tokenizer.texts_to_sequences(train_df.text), maxlen=SEQUENCE_LENGTH\n",
        "    )\n",
        "    logging.info(\"---- Tokenizing eval data ----\")\n",
        "    x_eval = pad_sequences(\n",
        "        tokenizer.texts_to_sequences(eval_df.text), maxlen=SEQUENCE_LENGTH\n",
        "    )\n",
        "\n",
        "    # Label Encoder\n",
        "    logging.info(\"---- Generating label encoder ----\")\n",
        "    label_encoder = generate_label_encoder(train_df)\n",
        "\n",
        "    logging.info(\"---- Encoding train target ----\")\n",
        "    y_train = label_encoder.transform(train_df.sentiment.tolist())\n",
        "    logging.info(\"---- Encoding eval target ----\")\n",
        "    y_eval = label_encoder.transform(eval_df.sentiment.tolist())\n",
        "\n",
        "    y_train = y_train.reshape(-1, 1)\n",
        "    y_eval = y_eval.reshape(-1, 1)\n",
        "\n",
        "    # Create Embedding Layer\n",
        "    logging.info(\"---- Generating embedding layer ----\")\n",
        "    embedding_layer = generate_embedding(word2vec_model, vocab_size, tokenizer)\n",
        "\n",
        "    logging.info(\"---- Generating Sequential model ----\")\n",
        "    model = Sequential()\n",
        "    model.add(embedding_layer)\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\n",
        "    model.add(Dense(1, activation=\"sigmoid\"))\n",
        "\n",
        "    model.summary()\n",
        "\n",
        "    logging.info(\"---- Adding loss function to model ----\")\n",
        "    model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
        "\n",
        "    logging.info(\"---- Adding callbacks to model ----\")\n",
        "    callbacks = [\n",
        "        ReduceLROnPlateau(monitor=\"val_loss\", patience=5, cooldown=0),\n",
        "        EarlyStopping(monitor=\"val_accuracy\", min_delta=1e-4, patience=5),\n",
        "    ]\n",
        "\n",
        "    logging.info(\"---- Training model ----\")\n",
        "    model.fit(\n",
        "        x_train,\n",
        "        y_train,\n",
        "        batch_size=batch_size,\n",
        "        steps_per_epoch=steps,\n",
        "        epochs=epochs,\n",
        "        validation_split=0.1,\n",
        "        verbose=1,\n",
        "        callbacks=callbacks,\n",
        "    )\n",
        "\n",
        "    logging.info(\"---- Evaluating model ----\")\n",
        "    score = model.evaluate(x_eval, y_eval, batch_size=batch_size)\n",
        "    logging.info(f\"ACCURACY: {score[1]}\")\n",
        "    logging.info(f\"LOSS: {score[0]}\")\n",
        "\n",
        "    logging.info(\"---- Saving models ----\")\n",
        "    pickle.dump(\n",
        "        tokenizer,\n",
        "        tf.io.gfile.GFile(os.path.join(model_dir, TOKENIZER_MODEL), mode=\"wb\"),\n",
        "        protocol=0,\n",
        "    )\n",
        "    with tempfile.NamedTemporaryFile(suffix=\".h5\") as local_file:\n",
        "        with tf.io.gfile.GFile(\n",
        "            os.path.join(model_dir, KERAS_MODEL), mode=\"wb\"\n",
        "        ) as gcs_file:\n",
        "            model.save(local_file.name)\n",
        "            gcs_file.write(local_file.read())\n",
        "\n",
        "    # word2vec_model.save(os.path.join(model_dir, WORD2VEC_MODEL))\n",
        "\n",
        "    # pickle.dump(\n",
        "    #     label_encoder, open(os.path.join(model_dir, ENCODER_MODEL), \"wb\"), protocol=0\n",
        "    # )\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    \"\"\"Main function called by AI Platform.\"\"\"\n",
        "\n",
        "    logging.getLogger().setLevel(logging.INFO)\n",
        "\n",
        "    parser = argparse.ArgumentParser(\n",
        "        formatter_class=argparse.ArgumentDefaultsHelpFormatter\n",
        "    )\n",
        "\n",
        "    parser.add_argument(\n",
        "        \"--job-dir\",\n",
        "        help=\"Directory for staging trainer files. \"\n",
        "        \"This can be a Google Cloud Storage path.\",\n",
        "    )\n",
        "\n",
        "    parser.add_argument(\n",
        "        \"--work-dir\",\n",
        "        required=True,\n",
        "        help=\"Directory for staging and working files. \"\n",
        "        \"This can be a Google Cloud Storage path.\",\n",
        "    )\n",
        "\n",
        "    parser.add_argument(\n",
        "        \"--batch-size\",\n",
        "        type=int,\n",
        "        default=1024,\n",
        "        help=\"Batch size for training and evaluation.\",\n",
        "    )\n",
        "\n",
        "    parser.add_argument(\n",
        "        \"--epochs\", type=int, default=8, help=\"Number of epochs to train the model\",\n",
        "    )\n",
        "\n",
        "    parser.add_argument(\n",
        "        \"--steps\",\n",
        "        type=int,\n",
        "        default=1000,\n",
        "        help=\"Number of steps per epoch to train the model\",\n",
        "    )\n",
        "\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    train_data_files = tf.io.gfile.glob(\n",
        "        os.path.join(args.work_dir, \"transformed_data/train/part-*\")\n",
        "    )\n",
        "    eval_data_files = tf.io.gfile.glob(\n",
        "        os.path.join(args.work_dir, \"transformed_data/eval/part-*\")\n",
        "    )\n",
        "\n",
        "    train_df = pd.concat(\n",
        "        [\n",
        "            pd.read_csv(\n",
        "                f,\n",
        "                names=[\"text\", \"sentiment\"],\n",
        "                dtype={\"text\": \"str\", \"sentiment\": \"str\"},\n",
        "            )\n",
        "            for f in train_data_files\n",
        "        ]\n",
        "    ).dropna()\n",
        "\n",
        "    eval_df = pd.concat(\n",
        "        [\n",
        "            pd.read_csv(\n",
        "                f,\n",
        "                names=[\"text\", \"sentiment\"],\n",
        "                dtype={\"text\": \"str\", \"sentiment\": \"str\"},\n",
        "            )\n",
        "            for f in eval_data_files\n",
        "        ]\n",
        "    ).dropna()\n",
        "\n",
        "    train_and_evaluate(\n",
        "        args.work_dir,\n",
        "        train_df=train_df,\n",
        "        eval_df=eval_df,\n",
        "        batch_size=args.batch_size,\n",
        "        epochs=args.epochs,\n",
        "        steps=args.steps,\n",
        "    )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zghmsyvNB5Hh",
        "outputId": "41260bb4-e4a5-4f48-a451-6a49c2d31e78"
      },
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting trainer/task.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8axPHdHX6d9W"
      },
      "source": [
        "### Validación Train en local\n",
        "\n",
        "Con el comando mostrado a continuación se valida el correcto entrenamiento del modelo usando los datos preprocesados del apartado anterior."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 103,
      "metadata": {
        "id": "m9997ZzsLmq-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "92425679-5675-4200-fc78-881d0f6f9763"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Updated property [ml_engine/local_python].\n",
            "2024-04-06 16:24:46.388594: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-04-06 16:24:46.388670: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-04-06 16:24:46.390285: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-04-06 16:24:47.778952: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n",
            "    return _run_code(code, main_globals, None,\n",
            "  File \"/usr/lib/python3.10/runpy.py\", line 86, in _run_code\n",
            "    exec(code, run_globals)\n",
            "  File \"/content/batch/twitter-sentiment-batch/trainer/task.py\", line 264, in <module>\n",
            "    train_df = pd.concat(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pandas/core/reshape/concat.py\", line 372, in concat\n",
            "    op = _Concatenator(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pandas/core/reshape/concat.py\", line 429, in __init__\n",
            "    raise ValueError(\"No objects to concatenate\")\n",
            "ValueError: No objects to concatenate\n"
          ]
        }
      ],
      "source": [
        "# Explicitly tell `gcloud ai-platform local train` to use Python 3\n",
        "! gcloud config set ml_engine/local_python $(which python3)\n",
        "\n",
        "# This is similar to `python -m trainer.task --job-dir local-training-output`\n",
        "# but it better replicates the AI Platform environment, especially for\n",
        "# distributed training (not applicable here).\n",
        "! gcloud ai-platform local train \\\n",
        "  --package-path trainer \\\n",
        "  --module-name trainer.task \\\n",
        "  -- \\\n",
        "  --work-dir $WORK_DIR \\\n",
        "  --epochs 1\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0nhlkcjn62Zo"
      },
      "source": [
        "## Tercer ejercicio\n",
        "\n",
        "Desarrollar un pipeline de inferencia utilizando Apache Beam para generar predicciones usando los modelos generados en el apartado anterior así como los datos de test generados en el primer ejercicio.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 104,
      "metadata": {
        "id": "eHBpLXB-OmjT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "98b4e209-027b-4857-df32-bf2a788a36af"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting predict.py\n"
          ]
        }
      ],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile predict.py\n",
        "\n",
        "from __future__ import absolute_import\n",
        "from __future__ import print_function\n",
        "\n",
        "import argparse\n",
        "import tempfile\n",
        "import json\n",
        "import os\n",
        "import sys\n",
        "import time\n",
        "\n",
        "import apache_beam as beam\n",
        "from apache_beam.io import ReadFromText\n",
        "from apache_beam.io import WriteToText\n",
        "from apache_beam.options.pipeline_options import GoogleCloudOptions\n",
        "from apache_beam.options.pipeline_options import PipelineOptions\n",
        "from apache_beam.options.pipeline_options import SetupOptions\n",
        "from apache_beam.coders.coders import Coder\n",
        "\n",
        "import pickle\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# KERAS\n",
        "SEQUENCE_LENGTH = 300\n",
        "\n",
        "# SENTIMENT\n",
        "POSITIVE = \"TROLL\"\n",
        "NEGATIVE = \"NO_TROLL\"\n",
        "# NEUTRAL = \"NEUTRAL\"\n",
        "# SENTIMENT_THRESHOLDS = (0.4, 0.7)\n",
        "\n",
        "# EXPORT\n",
        "KERAS_MODEL = \"model.h5\"\n",
        "TOKENIZER_MODEL = \"tokenizer.pkl\"\n",
        "\n",
        "\n",
        "class Predict(beam.DoFn):\n",
        "    def __init__(\n",
        "        self, model_dir,\n",
        "    ):\n",
        "        self.model_dir = model_dir\n",
        "        self.model = None\n",
        "        self.tokenizer = None\n",
        "\n",
        "    def setup(self):\n",
        "        keras_model_path = os.path.join(self.model_dir, KERAS_MODEL)\n",
        "        with tempfile.NamedTemporaryFile(suffix=\".h5\") as local_file:\n",
        "            with tf.io.gfile.GFile(keras_model_path, mode=\"rb\") as gcs_file:\n",
        "                local_file.write(gcs_file.read())\n",
        "                self.model = tf.keras.models.load_model(local_file.name)\n",
        "\n",
        "        tokenizer_path = os.path.join(self.model_dir, TOKENIZER_MODEL)\n",
        "        self.tokenizer = pickle.load(tf.io.gfile.GFile(tokenizer_path, mode=\"rb\"))\n",
        "\n",
        "    def decode_sentiment(self, score, include_neutral=True):\n",
        "        return NEGATIVE if score < 0.5 else POSITIVE\n",
        "\n",
        "    def process(self, element):\n",
        "        start_at = time.time()\n",
        "        # Tokenize text\n",
        "        x_test = pad_sequences(\n",
        "            self.tokenizer.texts_to_sequences([element]), maxlen=SEQUENCE_LENGTH\n",
        "        )\n",
        "        # Predict\n",
        "        score = self.model.predict([x_test])[0]\n",
        "        # Decode sentiment\n",
        "        label = self.decode_sentiment(score)\n",
        "\n",
        "        yield {\n",
        "            \"text\": element,\n",
        "            \"label\": label,\n",
        "            \"score\": float(score),\n",
        "            \"elapsed_time\": time.time() - start_at,\n",
        "        }\n",
        "\n",
        "\n",
        "class CustomCoder(Coder):\n",
        "    \"\"\"A custom coder used for reading and writing strings\"\"\"\n",
        "\n",
        "    def __init__(self, encoding: str):\n",
        "        # latin-1\n",
        "        # iso-8859-1\n",
        "        self.enconding = encoding\n",
        "\n",
        "    def encode(self, value):\n",
        "        return value.encode(self.enconding)\n",
        "\n",
        "    def decode(self, value):\n",
        "        return value.decode(self.enconding)\n",
        "\n",
        "    def is_deterministic(self):\n",
        "        return True\n",
        "\n",
        "\n",
        "def run(model_dir, source, sink, beam_options=None):\n",
        "    with beam.Pipeline(options=beam_options) as p:\n",
        "        _ = (\n",
        "            p\n",
        "            | \"Read data\" >> source\n",
        "            # | \"Preprocess\" >> beam.ParDo(PreprocessTextFn(model_dir, \"ID\"))\n",
        "            | \"Predict\" >> beam.ParDo(Predict(model_dir))\n",
        "            | \"Format as JSON\" >> beam.Map(json.dumps)\n",
        "            | \"Write predictions\" >> sink\n",
        "        )\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    \"\"\"Main function\"\"\"\n",
        "    parser = argparse.ArgumentParser(\n",
        "        formatter_class=argparse.ArgumentDefaultsHelpFormatter\n",
        "    )\n",
        "\n",
        "    parser.add_argument(\n",
        "        \"--work-dir\",\n",
        "        dest=\"work_dir\",\n",
        "        required=True,\n",
        "        help=\"Directory for temporary files and preprocessed datasets to. \"\n",
        "        \"This can be a Google Cloud Storage path.\",\n",
        "    )\n",
        "\n",
        "    parser.add_argument(\n",
        "        \"--model-dir\",\n",
        "        dest=\"model_dir\",\n",
        "        required=True,\n",
        "        help=\"Path to the exported TensorFlow model. \"\n",
        "        \"This can be a Google Cloud Storage path.\",\n",
        "    )\n",
        "\n",
        "    verbs = parser.add_subparsers(dest=\"verb\")\n",
        "    batch_verb = verbs.add_parser(\"batch\", help=\"Batch prediction\")\n",
        "    batch_verb.add_argument(\n",
        "        \"--inputs-dir\",\n",
        "        dest=\"inputs_dir\",\n",
        "        required=True,\n",
        "        help=\"Input directory where CSV data files are read from. \"\n",
        "        \"This can be a Google Cloud Storage path.\",\n",
        "    )\n",
        "    batch_verb.add_argument(\n",
        "        \"--outputs-dir\",\n",
        "        dest=\"outputs_dir\",\n",
        "        required=True,\n",
        "        help=\"Directory to store prediction results. \"\n",
        "        \"This can be a Google Cloud Storage path.\",\n",
        "    )\n",
        "\n",
        "    args, pipeline_args = parser.parse_known_args()\n",
        "    print(args)\n",
        "    beam_options = PipelineOptions(pipeline_args)\n",
        "    beam_options.view_as(SetupOptions).save_main_session = True\n",
        "    # beam_options.view_as(DirectOptions).direct_num_workers = 0\n",
        "\n",
        "    project = beam_options.view_as(GoogleCloudOptions).project\n",
        "\n",
        "    if args.verb == \"batch\":\n",
        "        results_prefix = os.path.join(args.outputs_dir, \"part\")\n",
        "\n",
        "        source = ReadFromText(args.inputs_dir, coder=CustomCoder(\"latin-1\"))\n",
        "        sink = WriteToText(results_prefix)\n",
        "\n",
        "    else:\n",
        "        parser.print_usage()\n",
        "        sys.exit(1)\n",
        "\n",
        "    run(args.model_dir, source, sink, beam_options)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5wEG8Uk9C8qd",
        "outputId": "fd64e829-e3b6-429e-ed80-16f2d40b9615"
      },
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting predict.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MUdgcLIP7a42"
      },
      "source": [
        "Generamos un timestamp para la ejecución de las predicciones"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 107,
      "metadata": {
        "id": "_pvKMtuQPOlr"
      },
      "outputs": [],
      "source": [
        "from datetime import datetime\n",
        "\n",
        "# current date and time\n",
        "TIMESTAMP = datetime.now().strftime('%Y-%m-%d_%H-%M-%S')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RkLMdMup70ne"
      },
      "source": [
        "### Validación Predict en local\n",
        "\n",
        "Con el comando mostrado a continuación se valida la correcta inferencia usando los modelos anteriores y los datos de test generados anteriormente."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 108,
      "metadata": {
        "id": "iUJN0XCLPR_X",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f18ecebe-10e6-4daa-f99b-84f8da99ef37"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-04-06 16:27:14.651463: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-04-06 16:27:14.651535: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-04-06 16:27:14.653250: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-04-06 16:27:15.955464: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Namespace(work_dir='/content/batch/twitter-sentiment-batch', model_dir='/content/batch/twitter-sentiment-batch/data/model', verb='batch', inputs_dir='/content/batch/twitter-sentiment-batch/data/transformed_data/test/part*', outputs_dir='/content/batch/twitter-sentiment-batch/predictions/2024-04-06_16-25-29')\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/batch/twitter-sentiment-batch/predict.py\", line 158, in <module>\n",
            "    source = ReadFromText(args.inputs_dir, coder=CustomCoder(\"latin-1\"))\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/apache_beam/io/textio.py\", line 781, in __init__\n",
            "    self._source = self._source_class(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/apache_beam/io/textio.py\", line 140, in __init__\n",
            "    super().__init__(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/apache_beam/io/filebasedsource.py\", line 127, in __init__\n",
            "    self._validate()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/apache_beam/options/value_provider.py\", line 193, in _f\n",
            "    return fnc(self, *args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/apache_beam/io/filebasedsource.py\", line 190, in _validate\n",
            "    raise IOError('No files found based on the file pattern %s' % pattern)\n",
            "OSError: No files found based on the file pattern /content/batch/twitter-sentiment-batch/data/transformed_data/test/part*\n"
          ]
        }
      ],
      "source": [
        "! python3 predict.py \\\n",
        "  --work-dir $WORK_DIR \\\n",
        "  --model-dir $WORK_DIR/data/model \\\n",
        "  batch \\\n",
        "  --inputs-dir $WORK_DIR/data/transformed_data/test/part* \\\n",
        "  --outputs-dir $WORK_DIR/predictions/$TIMESTAMP"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "co3YBKjscCVI"
      },
      "source": [
        "# Mensaje final\n",
        "\n",
        "¡Muchas gracias por participar en este curso, espero que tanto las sesiones teóricas como la práctica te hayan resultado útiles. A lo largo de esta semmana iréis recibiendo feedback personalizado sobre vuestras prácticas.\n",
        "\n",
        "¡Muchas gracias y ánimo con el proyecto final!"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}